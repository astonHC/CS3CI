// COPYRIGHT (C) HARRY CLARK 2025
// CS3_CI PARTICLE SWARM OPTIMISATION FOR GENETIC PROGRAMMING

// THIS FILE PERTAINS TOWARDS THE PSO ALGORITHM FOR OPTIMISING
// GENETIC PROGRAMMING PARAMETERS AND FITNESS EVALUATION

// SPECIFICALLY, THIS VERSION OF THE PSO IMPLEMENTATION
// AIMS TO PROVIDE AN UNDERSTANDING FOR HOW TO MITIGATE AN ISSUE
// RELATED TO THE DAILY DEMAND OF A LOGISTICS COMPANY

// THIS VERSION FOCUSSES ON THE SIMPLIFICATION OF THE CURRENT
// PSO IMPLEMENTATION - TO PROVIDE A POINT OF COMPARISON TO PROPERLY
// DISCERN THE BENEFITS OF EITHER OR

// NESTED INCLUDES

#include "pso_simple.h"

// IMPLEMENT A BASE FORM OF MEAN SQUARED ERROR TO BE ABLE TO
// GARNER A SEMBLENCE FOR THE OVERALL FITNESS
//
// PREUPPOSES THAT THE FOLLOWING DATASET ALLOWS
// FOR THE CURRENT DEMAND FITNESS
//
// AS THE NAME IMPLIES, THE TOTAL ERROR WILL
// BE THE ERROR TIMES BY ITSELF BASED ON OBSERVED AND PREDICTED RESULTS
double PSO_BASE_FITNESS(const double* POSITION, int DIMENSIONS, void* USER_DATA)
{
    // ACCESS FOR THE CURRENT FUNCTION POINTER FOR FITNESS
    PS STATE = (PS)USER_DATA;
    double TOTAL_ERROR = 0.0;
    double PREDICTION = 0.0;
    int COUNTER = 0;

    // VALIDATE IF WE HAVE ANY CONCURRENT DATA WITHIN THE SET
    // PREVENTS THOSE NASTY SPILLOVERS
    if(STATE->DATASET.SIZE == 0)
    {
        PSO_ERROR_HANDLE(OOB, PSO_ERROR_OOB, "DATASET IS EMTPY\n", "");
        return DBL_MAX;
    }

    // CALCULATE PREDICTED DEMAND BASED ON THE LINEAR COMBINATION
    // OF THE BIAS + WEIGHTED INDICATORS
    for(int INDEX = 0; INDEX < STATE->DATASET.SIZE; INDEX++)
    {
        PSO_DEMAND* ENTRY = &STATE->DATASET.DATA[INDEX];

        // CREATE A LINEAR COMBINATION BASED ON
        // HOW THE PREDICTION WORKS - INDICATORS AGAINST BIAS
        PREDICTION = POSITION[0];

        for(int ITERATOR = 0; ITERATOR < PSO_MAX_IND; ITERATOR++)
        {
            // BIAS TERM TO ACCOUNT FOR ALL POSSIBLE BIASES
            // AGAINST INDICATORS (0 - 13)
            if(ITERATOR + 1 < DIMENSIONS)
            {
                PREDICTION += POSITION[ITERATOR + 1] * ENTRY->INDICATIONS[ITERATOR];
            }
        }

        // CLAMP NEGATIVE PREDCITIONS TO ZERO
        // TO PREVENT SPILLOVER AND DIV BY ZERO
        if(PREDICTION < 0.0) PREDICTION = 0.0;

        double FINAL_ERROR = fabs(PREDICTION - ENTRY->DEMANDS);
        TOTAL_ERROR += FINAL_ERROR;
        COUNTER++;
    }

    return (COUNTER > 0) ? (TOTAL_ERROR / COUNTER) : DBL_MAX;
}

// INITIALISE THE PSO STATE BASE WITH IT'S DESIGNATED PARAMETERS SUCH AS 
// DIMENSIONS AND FITNESS TYPE, SWARM, BOUNDARIES, ETC 
int PSO_SIMPLE_INIT(PS STATE, int DIMENSIONS)
{
    // CHECK IF WE HAVE VALID DIMENSIONS IN ACCORDANCE WITH 
    // THE TRAINING DATA
    if(!PSO_VALID_DIM(DIMENSIONS, PSO_MAX_DEM + 1))
    {
        PSO_ERROR_HANDLE(DIM, PSO_ERROR_DIM, 
            "INVALID DIMENSION COUNT: %d (MAX: %d)", 
            DIMENSIONS, PSO_MAX_DEM);
        return 1;
    }

    memset(STATE, 0, sizeof(*STATE));

    // NOW DEFINE ALL OF THE CORRESPONDENCE 
    // A LOT LESS TO DEFINE DUE TO THE BASELINE IMPL.
    STATE->DIMENSIONS = DIMENSIONS;
    STATE->SWARM.PARTICLES->COUNT = PSO_MAX_PARTICLES;
    STATE->SWARM.ITERATION = 0;
    STATE->FITNESS = PSO_BASE_FITNESS;
    STATE->USER_DATA = STATE;
    STATE->SWARM.GBEST_FITNESS = DBL_MAX;
    
    PSO_HANDLE(NONE, PSO_ERROR_NONE,
        "\nPSO:\n" 
        "PARTICLES: %d\n" 
        "DIMENSIONS: %d\n"
        "INERTIA: %.5f (FIXED)\n"
        "COGNITIVE: %.5f (FIXED)\n"
        "SOCIAL: %.5f (FIXED)\n" 
        "MAX_ITER: %d\n",
        PSO_MAX_PARTICLES, DIMENSIONS,
        PSO_SIMPLE_INERTIA, PSO_SIMPLE_COG,
        PSO_SIMPLE_SOC, PSO_MAX_ITER);

    return 0;
}

void PSO_SIMPLE_BOUNDS(PS STATE, int DIMENSIONS, double LOWER, double UPPER)
{
    if(!PSO_VALID_DIM(DIMENSIONS, STATE->DIMENSIONS))
    {
        PSO_ERROR_HANDLE(OOB, PSO_ERROR_OOB,
            "DIMENSION %d OUT OF BOUNDS -> MAX DIMENSIONS: %d", DIMENSIONS, PSO_MAX_DEM);
        return;
    }

    STATE->BOUNDS[DIMENSIONS].LOWER = LOWER;
    STATE->BOUNDS[DIMENSIONS].UPPER = UPPER;

    #if PSO_DEBUG
    PSO_HANDLE(NONE, PSO_ERROR_NONE, 
        "DIMENSIONS SET WITH BOUNDS: [L]: %.2f -> [R]: %.2f\n", 
        LOWER, UPPER);
    #endif
}

// INITIALISE A SINGLE PARTICLE WITH A RANDOM POSITION AND VELOCITY
/// WITHIN THE SWARM - PRESUPPOSES A PERSONAL BEST AFTER THE FACT
static void PSO_SIMPLE_PARTICLE(PARTICLE P, const BOUNDS B, int DIMENSIONS)
{
    // ITERATE THROUGH EACH RESPECTIVE POSSIBLE RANGE FOR 
    // WHICH A PARTICLE WITHIN THE SWARM CAN EXIST
    //
    // DEFINE THE POSITION AND VELOCITY FOR SUCH
    for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
    {
        double PARTICLE_RANGE = B[INDEX].UPPER - B[INDEX].LOWER;
        double VELOCITY_MAX = PSO_VELO_DELTA * PARTICLE_RANGE;

        P->POSITION[INDEX] = B[INDEX].LOWER + (PSO_RAND() * PARTICLE_RANGE);
        P->VELOCITY[INDEX] = (PSO_RAND() * (2.0 * VELOCITY_MAX)) - VELOCITY_MAX;
        P->PBEST[INDEX] = P->POSITION[INDEX];
    } 

    P->PBEST_FITNESS = DBL_MAX;
    P->CURRENT_FITNESS = DBL_MAX;
    P->STAGNATE = 0;
}

// CREATE A RANDOM SEED FOR THE ALL ENCOMPASSING SWARM
// RATHER THAN BEING DELEGATED ON THE BASIS OF A LINEAR REPRESENTATION OF 
// PARTICLES, THIS SEED WILL BE RANDOMISED IN ACCORDANCE WITH THE TEST DATA
static void PSO_SIMPLE_SWARM(PS STATE)
{   
    PSO_SEED();

    // ITERATE THROUGH ALL OF THE CORRESPONDENCE OF THE AMOUNT
    // OF PARTICLES WITHIN THE SWARM
    // ITERATE THROUGH THE MAX PARTICLES AND ASSIGN THEM
    for(int INDEX = 0; INDEX < PSO_MAX_PARTICLES; INDEX++)
    {
        PARTICLE P = &STATE->SWARM.PARTICLES[INDEX];
        PSO_SIMPLE_PARTICLE(P, STATE->BOUNDS, STATE->DIMENSIONS);
    }

    PSO_HANDLE(NONE, PSO_ERROR_NONE, "SWARM INIT WITH %d PARTICLES\n", PSO_MAX_PARTICLES);
}

// UPDATES THE INERTIA VELOCITY OF THE SWARM
// BASD ON THE FIXED INERTIA AND COMPONENT PARAMETERS
static void PSO_UPDATE_INERTIA(PARTICLE P, const double* GBEST, int DIMENSIONS)
{
    double COGNITIVE = 0.0;
    double SOCIAL = 0.0;

    for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
    {
        double COG_RAND = PSO_RAND();
        double SOC_RAND = PSO_RAND();

        // NOW UPDATE THE INERTIA VELOCITY BASSD ON THE FORMULA 
        // WITH DUE REGARD FOR FIXED COEFFICIENTS 
        
        COGNITIVE = PSO_SIMPLE_COG * COG_RAND * (P->PBEST[INDEX] - P->POSITION[INDEX]);
        SOCIAL = PSO_SIMPLE_SOC * SOC_RAND * (GBEST[INDEX] - P->POSITION[INDEX]);

        // APPLY THE FIXED COEFFICIENTS TO THE VELOCITY BASED ON COMPONENTS
        P->VELOCITY[INDEX] = (PSO_SIMPLE_INERTIA * P->VELOCITY[INDEX]) + COGNITIVE + SOCIAL;
    }
}

// NOW DO THE SAME BUT FOR POSITION
// THE DIFFERENCE HERE BEING IS THAT WE WANT
// TO ENSURE PROPER CLAMPING OF POSITIONS IN ACCORDANCE WITH THE BOUNDS
static void PSO_UPDATE_POS(PARTICLE P, const BOUNDS B, int DIMENSIONS)
{
    for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
    {
        P->POSITION[INDEX] += P->VELOCITY[INDEX];

        // SET BOUNDS PROPERLY
        // DETERMINE THE VELOCITY FOR EACH TO LEVERAGE 
        // A BIAS FOR HANDLING THE WEIGHTS
        if(P->POSITION[INDEX] < B[INDEX].LOWER)
        {
            P->POSITION[INDEX] = B[INDEX].LOWER;
            P->VELOCITY[INDEX] *= PSO_INER_DAMP;
        }
        else if(P->POSITION[INDEX] > B[INDEX].UPPER)
        {
            P->POSITION[INDEX] = B[INDEX].UPPER;
            P->VELOCITY[INDEX] *= PSO_INER_DAMP;
        }
    }
}

// UPDATE THE PERSONAL BEST BASED ON THE CURRENT FITNESS
static void PSO_UPDATE_PBEST(PARTICLE P, int DIMENSIONS)
{
    if(P->CURRENT_FITNESS < P->PBEST_FITNESS)
    {
        P->PBEST_FITNESS = P->CURRENT_FITNESS;

        // ITERATE THROUGH ALL OF THE DIMENSIONS
        // GIVEN TO THE SWARM
        for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
        {
            P->PBEST[INDEX] = P->POSITION[INDEX];
        }

        P->STAGNATE = 0;
    }
    else
    {
        P->STAGNATE++;
    }
}

// AND DO THE SAME FOR THE GLOBAL BEST
// FOR THIS, WE WANT TO BE ABLE TO DETERMINE THE CONVERGENCE RATE
// FOR THE CURRENT SWARM ITERATION
static void PSO_UPDATE_GBEST(PS STATE)
{
    int FOUND = 0;

    for(int INDEX = 0; INDEX < PSO_MAX_PARTICLES; INDEX++)
    {
        PARTICLE P = &STATE->SWARM.PARTICLES[INDEX];

        // DO WE ACTUALLY HAVE A NEW BEST FITNESS?
        if(P->PBEST_FITNESS < STATE->SWARM.GBEST_FITNESS)
        {
            STATE->SWARM.GBEST_FITNESS = P->PBEST_FITNESS;
            STATE->STATS.CONVERGENCE_ITER = STATE->SWARM.ITERATION;
            FOUND = 1;

            #if PSO_DEBUG
            PSO_HANDLE(NONE, PSO_ERROR_NONE, 
                "NEW GLOBAL BEST FITNESS: %.2f AT ITERATION %d -> ORIGINAL FITNESS: %2.f", 
                P->PBEST_FITNESS, STATE->SWARM.ITERATION, P->CURRENT_FITNESS);
            #endif

            // BASED ON THE DIMENSIONS OF THE CURRENT SWARM, APPLY THE BEST TO SUCH
            for(int ITERATOR = 0; ITERATOR < STATE->DIMENSIONS; ITERATOR++)
            {
                STATE->SWARM.GBEST[ITERATOR] = P->PBEST[ITERATOR];
            }
        }
    }
}

// OPTIMISE THE ALGORITHM IN ACCORDANCE WITH THE EVALUATION
// OF THE FITNESS OF THE CURRENT INERTIA
void PSO_SIMPLE_OPTIMISE(PS STATE)
{
    PSO_SIMPLE_SWARM(STATE);

    // EVALUATE FITNESS BEFORE OPTIMISATION
    // THIS HELPS IN BEING ABLE TO PROPERLY DISCERN THE FITNESS
    // BEFORE ANYTHING ELSE - ALLOWING FOR A POINT OF COMPARISON
    for(int FITNESS_INDEX = 0; FITNESS_INDEX < PSO_MAX_PARTICLES; FITNESS_INDEX++)
    {
        PARTICLE P = &STATE->SWARM.PARTICLES[FITNESS_INDEX];
        P->CURRENT_FITNESS = STATE->FITNESS(P->POSITION, STATE->DIMENSIONS, STATE->USER_DATA);
        PSO_UPDATE_PBEST(P, STATE->DIMENSIONS);
    }

    // CREATE THE INITIAL BASIS FOR THE FITNESS EVALUATION
    // THE FOLLOWING WILL CREATE A PARTICLE TO ADD TO THE SWARM
    // BASED ON THE MAX, APPLY THE CURRENT FITNESS TO THE GLOBAL BEST 
    // AND UPDATE ALL OF THE CORRESPONDENCE

    PSO_UPDATE_GBEST(STATE);
    STATE->STATS.INIT_FITNESS = STATE->SWARM.GBEST_FITNESS;
    PSO_HANDLE(NONE, PSO_ERROR_NONE, "INITIAL BEST FITNESS FOR CURRENT SWARM ITERATION: %.6f\n", STATE->SWARM.GBEST_FITNESS);

    // NOW WE CAN ITERATE THROUGH EACH INSTANCE 
    // OF THE ALGORITHM IN ACCORDANCE WITH THE MAX AMOUNT
    // OF ITERATIONS

    for(int INDEX = 0; INDEX < PSO_MAX_ITER; INDEX++)
    {
        STATE->SWARM.ITERATION = INDEX;

        for(int PARTICLE_ITER = 0; PARTICLE_ITER < PSO_MAX_PARTICLES; PARTICLE_ITER++)
        {
            PARTICLE P = &STATE->SWARM.PARTICLES[PARTICLE_ITER];

            // UPDATE ALL OF THE CORRESPONDENCE WITH DUE REGARD
            // FOR THE FIXED COEFFICIENTS
            PSO_UPDATE_INERTIA(P, STATE->SWARM.GBEST, STATE->DIMENSIONS);
            PSO_UPDATE_POS(P, STATE->BOUNDS, STATE->DIMENSIONS);

            P->CURRENT_FITNESS = STATE->FITNESS(P->POSITION, STATE->DIMENSIONS, STATE->USER_DATA);
            PSO_UPDATE_PBEST(P, STATE->DIMENSIONS);
        }

        PSO_UPDATE_GBEST(STATE);
    }

    STATE->STATS.FINAL_FITNESS = STATE->SWARM.GBEST_FITNESS;
    
    // 06/11/25 - PREVENT DIV BY ZERO FOR LARGE SCALE VALUES
    if(STATE->STATS.INIT_FITNESS > 0)
    {
        STATE->STATS.IMPROVEMENT_RATE = (STATE->STATS.INIT_FITNESS - STATE->STATS.FINAL_FITNESS) /
                                        STATE->STATS.INIT_FITNESS * 100.0;
    }
    else
    {
        STATE->STATS.IMPROVEMENT_RATE = 0.0;
    }

    // NOW WE DETERMINE THE DIFFERENCE BETWEEN THE INITIAL BEST FITNESS
    // AND THE FINAL
    PSO_HANDLE(NONE, PSO_ERROR_NONE, "\n"
        "\nFINAL FITNESS: %.6f\n" 
        "IMPROVEMENT RATE: %.2f%% (FROM %.6f to %.6f)\n", 
        STATE->STATS.FINAL_FITNESS, STATE->STATS.IMPROVEMENT_RATE,
        STATE->STATS.INIT_FITNESS, STATE->STATS.FINAL_FITNESS);
}

// LOAD THE CSV FILE FOR WORKING WITH TRAIN AND TESTING DATA
int PSO_LOAD_CSV(PSO_DATASET* DATASET, const char* FILENAME)
{
    FILE* FILE_PTR = fopen(FILENAME, "r");
    if(!FILE_PTR) 
    { 
        PSO_ERROR_HANDLE(IO, PSO_ERROR_FILE, "FAILED TO OPEN FILE: %s", FILENAME); 
        return 1;
    }

    // DEFINE SOME REPRESENTATIONS FOR THE DATASET WITHIN EACH CSV
    DATASET->CAPACITY = PSO_MAX_CSV;
    DATASET->SIZE = 0;
    DATASET->DATA = (PSO_DEMAND*)malloc(DATASET->CAPACITY * sizeof(PSO_DEMAND));

    if(!DATASET->DATA)
    {
        fclose(FILE_PTR);
        PSO_ERROR_HANDLE(MEM, PSO_ERROR_MEM, "NO VALID DATA SETS WITHIN THE LOAD CSV FILE: %s", FILE_PTR);
        free(DATASET->DATA);
        return 1;
    }

    // ASSUME THAT THE MAX AMOUNT OF DATA AND OR ITERATIONS
    // WITHIN THE CSV IS LESS THAN THE MAX SWARM COUNT
    if(PSO_MAX_CSV < PSO_MAX_PARTICLES || PSO_MAX_CSV == PSO_MAX_PARTICLES)
    {
        PSO_ERROR_HANDLE(PART, PSO_ERROR_PARTICLE, 
            "COULD NOT PROCESS CSV DATA -> CSV CAPACITY: %d MAX PARTICLES: %d\n", 
            PSO_MAX_CSV, PSO_MAX_PARTICLES);
        return 1;
    }
    
    // MANUALLY READ EACH LINE OF THE CSV
    // PRESUPPOSES THE SIZE OF EACH LINE BEING READ
    // IN RELATION TO EACH NEW LINE WITHIN THE FILE
    char LINE[PSO_CSV_BUFFER];
    fgets(LINE, sizeof(LINE), FILE_PTR);
    
    while(fgets(LINE, sizeof(LINE), FILE_PTR))
    {
        PSO_DEMAND* ENTRY = &DATASET->DATA[DATASET->SIZE];
        
        // MANUALLY MITIGATE THE HEADER LINES
        // THE FOLLOWING WILL LOOK TO DETERMINE WHICH
        // AREAS OF THE HEADER ENCOMPASS ANY AND ALL WHITESPACE
        char* TOKEN = strtok(LINE, ",");
        if(!TOKEN) continue;

        ENTRY->DEMANDS = atof(TOKEN);
        
        // LOOK FOR THE MAX AMOUNT OF INDICATORS
        // AND PARSE THEM - ADD THEM TO THE RESPECTIBVE
        // BUFFER ENTRY
        int VALID_ENTRY = 1;
        for(int i = 0; i < PSO_MAX_IND; i++)
        {
            TOKEN = strtok(NULL, ",");
            if(!VALID_ENTRY) { VALID_ENTRY = 0; break; }

            ENTRY->INDICATIONS[i] = atof(TOKEN);
        }
        
        if(VALID_ENTRY)
        {
            DATASET->SIZE++;
        }
    }
    
    fclose(FILE_PTR);
    PSO_HANDLE(NONE, PSO_ERROR_NONE, 
        "\nLOADED %d RECORDS FROM %s\n", DATASET->SIZE, FILENAME);
    
    return 0;
}
