// COPYRIGHT (C) HARRY CLARK 2025
// CS3_CI PARTICLE SWARM OPTIMISATION FOR GENETIC PROGRAMMING

// THIS FILE PERTAINS TOWARDS THE PSO ALGORITHM FOR OPTIMISING
// GENETIC PROGRAMMING PARAMETERS AND FITNESS EVALUATION

// SPECIFICALLY, THIS VERSION OF THE PSO IMPLEMENTATION
// AIMS TO PROVIDE AN UNDERSTANDING FOR HOW TO MITIGATE AN ISSUE
// RELATED TO THE DAILY DEMAND OF A LOGISTICS COMPANY

// LEVERAGING ADAPTIVE INERTIA CONTROL FOR GENERATING
// A BEST FIT SOLUTION BASED CURRENT FITNESS - MADE POSSIBLE BY DYNAMICALLY
// ADJUSTING THE WEIGHT OF PARTICLES BASED ON THEIR CHARACTERISTICS 
// DURING THE LIFETIME OF THE ALGORITHM

// NESTED INCLUDES

#include "pso.h"

// INITIALISE THE PSO STATE BASE WITH IT'S DESIGNATED PARAMETERS SUCH AS 
// DIMENSIONS AND FITNESS TYPE, SWARM, BOUNDARIES, ETC 
int PSO_INIT(PSO_STATE* STATE, int DIMENSIONS)
{
    // CHECK IF WE HAVE VALID DIMENSIONS IN ACCORDANCE WITH 
    // THE TRAINING DATA
    if(!PSO_VALID_DIM(DIMENSIONS, PSO_MAX_DEM + 1))
    {
        PSO_ERROR_HANDLE(DIM, PSO_ERROR_DIM, 
            "INVALID DIMENSION COUNT: %d (MAX: %d)", 
            DIMENSIONS, PSO_MAX_DEM);
        return 1;
    }

    memset(STATE, 0, sizeof(PSO_STATE));

    // NOW DEFINE ALL OF THE CORRESPONDENCE 
    STATE->DIMENSIONS = DIMENSIONS;
    STATE->CONV_THRESHOLD = DBL_MAX;
    STATE->SWARM.PARTICLES->COUNT = PSO_MAX_PARTICLES;
    STATE->SWARM.ITERATION = 0;
    STATE->SWARM.GBEST_FITNESS = DBL_MAX;
    STATE->SWARM.CONVERGED = 0;
    STATE->SWARM.DIVERSITY = 1.0;
    STATE->SWARM.STAGNATE = 0;
    
    // DEFINE ALL OF THE ADAPTIVE PARAMETERS
    STATE->CURRENT_INERTIA = PSO_MAX_INERTIA;
    STATE->CURRENT_COG = PSO_COG_INIT;
    STATE->CURRENT_SOC = PSO_SOC_INIT;

    PSO_HANDLE(NONE, PSO_ERROR_NONE, 
        "\nPSO:\n PARTICLES: %d\n DIMENSIONS: %d\n MAX_ITER: %d\n" 
        " MAX_CSV: %d\n",
        PSO_MAX_PARTICLES, DIMENSIONS, PSO_MAX_ITER, PSO_MAX_CSV);

    return 0;
}

// SET CUSTOM BOUNDS FOR A SPECIFIC DIMENSION
void PSO_SET_BOUNDS(PSO_STATE* STATE, int DIMENSION, double LOWER, double UPPER)
{
    if(!PSO_VALID_DIM(DIMENSION, STATE->DIMENSIONS))
    {
        PSO_ERROR_HANDLE(OOB, PSO_ERROR_OOB, 
            "DIMENSION %d OUT OF BOUNDS, MAX DIMENSION VALUE: %d", 
            DIMENSION, STATE->DIMENSIONS - 1);

        return;
    }

    // ASSIGN THE CORRESPONDING BOUNDS TO THE STATE 
    STATE->BOUNDS[DIMENSION].LOWER = LOWER;
    STATE->BOUNDS[DIMENSION].UPPER = UPPER;
}

// INITIALISE A SINGLE PARTICLE WITH A RANDOM POSITION AND VELOCITY
/// WITHIN THE SWARM - PRESUPPOSES A PERSONAL BEST AFTER THE FACT
static void PSO_INIT_PARTICLE(PSO_PARTICLE* PARTICLE, const PSO_BOUND* BOUNDS, int DIMENSIONS)
{
    // ITERATE THROUGH EACH RESPECTIVE POSSIBLE RANGE FOR 
    // WHICH A PARTICLE WITHIN THE SWARM CAN EXIST
    //
    // DEFINE THE POSITION AND VELOCITY FOR SUCH
    for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
    {
        double PARTCILE_RANGE = BOUNDS[INDEX].UPPER - BOUNDS[INDEX].LOWER;
        double VELOCITY_MAX = PSO_VELO_DELTA * PARTCILE_RANGE;

        PARTICLE->POSITION[INDEX] = BOUNDS[INDEX].LOWER + (PSO_RAND() * PARTCILE_RANGE);
        PARTICLE->VELOCITY[INDEX] = (PSO_RAND() * (2.0 * PSO_VELO_DELTA)) - PSO_VELO_DELTA;
        PARTICLE->PBEST[INDEX] = PARTICLE->POSITION[INDEX];
    } 

    PARTICLE->PBEST_FITNESS = DBL_MAX;
    PARTICLE->CURRENT_FITNESS = DBL_MAX;
    PARTICLE->STAGNATE = 0;
}

// CREATE A RANDOM SEED FOR THE ALL ENCOMPASSING SWARM
// RATHER THAN BEING DELEGATED ON THE BASIS OF A LINEAR REPRESENTATION OF 
// PARTICLES, THIS SEED WILL BE RANDOMISED IN ACCORDANCE WITH THE TEST DATA
//
// THE TEST DATA OF WHICH WILL BE THE DETERMINANT FOR HOW WELL THE SWARM PERFORMS
// IN ACCORDANCE WITH THE ADAPTIVE MEASURES
static void PSO_INIT_SWARM(PSO_STATE* STATE)
{
    PSO_SEED();

    // ITERATE THROUGH THE MAX PARTICLES AND ASSIGN THEM
    for(int INDEX = 0; INDEX < PSO_MAX_PARTICLES; INDEX++)
    {
        PSO_INIT_PARTICLE(&STATE->SWARM.PARTICLES[INDEX], STATE->BOUNDS, STATE->DIMENSIONS);
    }
}

// DEFINE THE BASIS BY WHICH WE ARE ABLE TO DETERMINE THE
// CURRENT DEMAND ON BEHALF OF THE LOGISTICS COMPANY OUTLINED IN THE BRIEF
//
// PRESUPPOSES THE CONTEXT OF THE CURRENT BIAS AS PER EACH PREDICTION
static double PSO_PREDICT(const double* PARAMS, const double* INDICATIONS)
{
    // THE PARAMS ENCOMPASSING EACH PREDICTION AGAINST THE BIAS (aN)
    double PREDICTION = PARAMS[0];

    for(int INDEX = 0; INDEX < PSO_MAX_IND; INDEX++)
    {
        PREDICTION += PARAMS[INDEX + 1] * INDICATIONS[INDEX];
    }

    #if PSO_DEBUG
    PSO_HANDLE(NONE, PSO_ERROR_NONE, "PREDICTION: %2.f", PREDICTION);
    #endif

    return PREDICTION;
}

// DEFINE THE BASIS BY WHICH WE ARE ABLE TO DETERMINE THE EFFECTIVENESS 
// OF THE DEMAND AND IT'S FITNESS
//
// THIS WILL BE UNDER THE PRETENSE OF USING MAE (THE REPRESENTATION OF AVERAGE ABSOLUTE ERROR)
//
// IN THEORY, THIS SHOULD ALLOW US TO GARNER A LOWER MARGIN OF ERROR
// TO HELP WITH CREATING LARGER PREDICTIONS TO MITIGATE OVERHEAD
double PSO_DEMAND_FITNESS(const double* PARAMS, const PSO_DATASET* DATASET)
{
    double TOTAL_ERROR = 0.0;
    double MAE = 0;
    double PREDICTION = 0.0;
    double ERROR = 0.0;

    // DO WE HAVE ANY VALID DATA TO BEGIN WITH?
    if(DATASET->SIZE == 0) 
    { 
        PSO_ERROR_HANDLE(OOB, PSO_ERROR_OOB, "DATASET IS OUT OF BOUNDS WITH SIZE: %d\n", DATASET->SIZE); 
    }

    // GARNER THE TOTAL ERROR ABSOLUTE VALUE
    // FROM THE PREDICITION IN THE DEMAND SET
    for(int INDEX = 0; INDEX < DATASET->SIZE; INDEX++)
    {
        PREDICTION = PSO_PREDICT(PARAMS, DATASET->DATA[INDEX].INDICATIONS);
        ERROR = fabs(PREDICTION - DATASET->DATA[INDEX].DEMANDS);
        
        // ASSIGN THE ABOVE TO THE TOTAL ERROR COST
        TOTAL_ERROR += ERROR;
    } 

    // ASSIGN THE AVERAGE ABSOLUTE AGAINST THE SIZE OF THE DATASET
    // THIS WILL DETERMINE THE EFFECTIVENESS OF THE SAVED COST
    MAE = TOTAL_ERROR / DATASET->SIZE;

    #if PSO_DEBUG
    PSO_HANDLE(NONE, PSO_ERROR_NONE, "AVERAGE ABSOLUTE ERROR: %.2f FOR PREDICTION: %.2f\n", MAE, PREDICTION);
    #endif
    
    return MAE;
}

// DETERMINE THE PLAUSIBILE AMOUNT OF SWARM DIVERSITY WITHIN THE CURRENT SWARM
// THIS WILL ASSYME THE AVERAGE DISTANCE BETWEEN ALL PARTICLE PAIRS
// TO EVAULATE THE SEARCH SPACE WITHIN EACH CLUSTER - DIVERSIFYING
// THE RANGE BY WHICH THE SWARM CAN BE EXTENDED TO
static double PSO_SWARM_DIVERSITY(STATE S)
{
    double AVG_DIST = 0.0;
    double DISTANCE = 0.0;
    double SEARCH_SPACE = 0.0;
    double SEARCH_RANGE = 0.0;
    int COMP = 0;
    
    // CALCULATE THE AVERAGE DISTANCE BETWEEN ALL CURRENT PAIRS
    for(int INDEX = 0; INDEX < PSO_MAX_PARTICLES; INDEX++)
    {
        for(int ITERATOR = INDEX + 1; ITERATOR < PSO_MAX_PARTICLES; ITERATOR++)
        {
            // RESET PROPERLY FOR EACH PAIR
            // IT IS ONLY ZEROED OUT BY DEFAULT BECAUSE OF C'S "GARBAGE VALUE INIT"
            DISTANCE = 0.0;

            // DEFINE THE EUCLIDEAN DISTANCE BETWEEN PARTICLES
            // HELPS WITH BEING ABLE TO GAIN A GREATER SENSE
            // OF CONVERGENCE
            for(int DIMENSIONS = 0; DIMENSIONS < S->DIMENSIONS; DIMENSIONS++)
            {
                double DIFFERENCE = S->SWARM.PARTICLES[INDEX].POSITION[DIMENSIONS] -
                                    S->SWARM.PARTICLES[ITERATOR].POSITION[DIMENSIONS];

                DISTANCE += DIFFERENCE * DIFFERENCE;
            }

            // APPLY EUC_DIST AGAINST THE DISTANCE BETWEEN EACH PAIR
            double EUC_DIST = sqrt(DISTANCE);
            AVG_DIST += EUC_DIST;
            COMP++;

            #if PSO_DEBUG
            PSO_HANDLE(DIM, PSO_ERROR_NONE, "DIFFERENCE BETWEEN SWARM DIVERSITY: %.6f\n", EUC_DIST);
            #endif
        }
    }

    return (COMP > 0) ? (AVG_DIST / COMP) : 0.0;
}

// UPDATE THE INERTIA VELOCITY OF THE CURRENT PARTICLE SWARM
// BASED ON THE FOLLOWING PRE-REQUISITIES:
//
// COGNITIVE - WHAT WAS THE PREVIOUS BEST POSITION?
// SOCIAL - WHAT WAS THE POPULATION'S OVERALL BEST POSITION?
//
// ALL WITH THE INTENT OF GUIDING THE MOVEMENT OF PARTICLES
// WITHIN THE SWARM BASED ON PRESUPPOSED MEMORY 
static void PSO_UPDATE_INERTIA(PARTICLE P, const double* GBEST, int DIMENSIONS, double INERTIA, double COG, double SOC)
{
    double SOCIAL_RAND = 0;
    double COGNITIVE_RAND = 0;
    double COG_COMP = 0;
    double SOC_COMP = 0;

    for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
    {
        SOCIAL_RAND = PSO_RAND();
        COGNITIVE_RAND = PSO_RAND();

        // ASSIGN THE COMPONENTS OF EACH TO THEIR RESPECTIVE FITNESS
        COG_COMP = COG * COGNITIVE_RAND * (P->PBEST[INDEX] - P->POSITION[INDEX]);
        SOC_COMP = SOC * SOCIAL_RAND * (GBEST[INDEX] - P->POSITION[INDEX]);

        // ASSIGN THE VELOCITY TO THE COMPONENTS IN ACCORDANCE WITH THE INERTIA
        // THE INERTIA WILL BE HANDLED BASED ON THE TWO ACTING COMPONENTS  
        P->VELOCITY[INDEX] = (INERTIA * P->VELOCITY[INDEX]) + COG_COMP + SOC_COMP;
    }
}

// NOW DO THE SAME BUT FOR POSITION
// THE DIFFERENCE HERE BEING IS THAT WE WANT
// TO ENSURE PROPER CLAMPING OF POSITIONS IN ACCORDANCE WITH THE BOUNDS
static void PSO_UPDATE_POS(PARTICLE P, const PSO_BOUND* BOUNDS, int DIMENSIONS)
{
    for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
    {
        P->POSITION[INDEX] += P->VELOCITY[INDEX];

        // SET BOUNDS PROPERLY
        // DETERMINE THE VELOCITY FOR EACH TO LEVERAGE 
        // A BIAS FOR HANDLING THE WEIGHTS
        if(P->POSITION[INDEX] < BOUNDS[INDEX].LOWER)
        {
            P->POSITION[INDEX] = BOUNDS[INDEX].LOWER;
            P->VELOCITY[INDEX] *= PSO_INER_DAMP;
        }
        else if(P->POSITION[INDEX] > BOUNDS[INDEX].UPPER)
        {
            P->POSITION[INDEX] = BOUNDS[INDEX].UPPER;
            P->VELOCITY[INDEX] *= PSO_INER_DAMP;
        }
    }
}

// UPDATE THE PERSONAL BEST FOR THE CURRENT FITNESS
// BASED ON THE POSITIONS OF THE PARTICLES IN THE SWARM 
static void PSO_UPDATE_PBEST(PARTICLE P, int DIMENSIONS)
{
    // DETERMINE IF THE CURRENT FITNESS IS NOT
    // AS GOOD AS THE PERSONAL BEST 
    if(P->CURRENT_FITNESS < P->PBEST_FITNESS)
    {
        P->PBEST_FITNESS = P->CURRENT_FITNESS;

        #if PSO_DEBUG
        PSO_HANDLE(NONE, PSO_ERROR_NONE, 
            "NEW PERSONAL BEST FITNESS: %.2f ORIGINAL FITNESS: %2.f", 
            P->PBEST_FITNESS, P->CURRENT_FITNESS);
        #endif

            // ASSIGN THE NEW BEST TO THE POSITION
            // OF THE PARTICLES IN THE SWARM
            for(int INDEX = 0; INDEX < DIMENSIONS; INDEX++)
            {
                P->PBEST[INDEX] = P->POSITION[INDEX];
            }

            P->STAGNATE = 0;
    }
    else
    {
        P->STAGNATE++;
    }
}

// UPDATE THE ADAPATIVE PARAMETERS TO HANDLE THE AMOUNT OF
// ITERATIONS CURRENTLY HAPPENING AGAINST THE INERTIA BIAS
//
// THIS IS AN ALL ENCOMPASSING FUNCTIONS DESIGNED
// TO HANDLE THE ADAPATIONS FOR THE NOVEL VARIANTS
static void PSO_UPDATE_ADAPT(STATE S)
{
    double PROGRESS = (double)S->SWARM.ITERATION / PSO_MAX_ITER;

    // DETERMINE THE CURRENT DIVERSITY BASED ON CURRENT SWARM CONVERGENCE
    S->SWARM.DIVERSITY = PSO_SWARM_DIVERSITY(S);

    // LEVERAGE A NON-LINEAR MEANS OF DETERMING THE ADAPTATION OVERTIME
    // HELPS WITH DOING LESS AGREGEIOUS CHECKS AGAINST COEFFICIENTS
    //
    // DETERMINE THE EXPONENT VALUE OF THE GRADUAL INCREASE IN THE LIFETIME
    // OF THE ADAPTATION
    double ADAPT = PSO_ADAPT_GRAD / (PSO_ADAPT_GRAD + exp(-PSO_ADAPT_PEAK * (PROGRESS - PSO_ADAPT_INTER)));

    // LINEARLY DECREASE THE INERTIA WEIGHT TO GARNER
    // A GREATER CONTROL AGAINST THE VELOCITY OF THE PARTICLES
    S->CURRENT_INERTIA = PSO_MAX_INERTIA - (PSO_MAX_INERTIA - PSO_MIN_INERTIA) * ADAPT;

    // MAP ALL OF THE COEFFICIENTS TO THE NOVEL COMPONENTS
    S->CURRENT_COG = PSO_COG_INIT - (PSO_COG_INIT - PSO_COG_FIN) * ADAPT;
    S->CURRENT_SOC = PSO_SOC_INIT + (PSO_SOC_FIN - PSO_SOC_INIT) * ADAPT;
}

// DO THE SAME AS ABOVE BY FOR THE GLOBAL BEST
static void PSO_UPDATE_GBEST(STATE S)
{
    double PREV_GBEST = S->SWARM.GBEST_FITNESS;
    int IMPROVEMENT = 0;
    int INDEX = 0;
    int ITERATOR = 0;

    // ITERATE THROUGH ALL OF THE CURRENT PARTICLES
    // IN THE SWARM - WE DO THIS TO ACCOUNT FOR THE 
    // SWARM AS A WHOLE RATHER THAN THE PERSONAL BEST
    // OF A CLUSTER

    for(INDEX = 0; INDEX < PSO_MAX_PARTICLES; INDEX++)
    {
        PARTICLE P = &S->SWARM.PARTICLES[INDEX];

        // DO WE ACTUALLY HAVE A NEW BEST FITNESS?
        if(P->PBEST_FITNESS < S->SWARM.GBEST_FITNESS)
        {
            S->SWARM.GBEST_FITNESS = P->PBEST_FITNESS;
            S->STATS.CONVERGENCE_ITER = S->SWARM.ITERATION;

            #if PSO_DEBUG
            PSO_HANDLE(NONE, PSO_ERROR_NONE, 
                "NEW GLOBAL BEST FITNESS: %.2f AT ITERATION %d -> ORIGINAL FITNESS: %2.f", 
                P->PBEST_FITNESS, S->SWARM.ITERATION, P->CURRENT_FITNESS);
            #endif

            // BASED ON THE DIMENSIONS OF THE CURRENT SWARM
            // APPLY THE BEST TO SUCH
            for(ITERATOR = 0; ITERATOR < S->DIMENSIONS; ITERATOR++)
            {
                S->SWARM.GBEST[ITERATOR] = P->PBEST[ITERATOR];
            }
        }
    }
}

// OPTIMISE THE ALGORITHM IN ACCORDANCE WITH THE EVALUATION
// OF THE FITNESS OF THE CURRENT INERTIA
void PSO_OPTIMISE(PSO_STATE* STATE)
{
    PSO_INIT_SWARM(STATE);

    // INITIAL FITNESS EVALUATION
    // GRABS THE CURRENT FITNESS OF THE DEMAND LEVEL BASED ON THE
    // INERTIA OF THE PARTICLES WITHIN THE SWARM AGAINST THE TEST DATA'S BIAS
    for(int INDEX = 0; INDEX < PSO_MAX_PARTICLES; INDEX++)
    {
        PARTICLE P = &STATE->SWARM.PARTICLES[INDEX];
        P->CURRENT_FITNESS = PSO_DEMAND_FITNESS(P->POSITION, &STATE->DATASET);

        PSO_UPDATE_PBEST(P, STATE->DIMENSIONS);
    }

    PSO_UPDATE_GBEST(STATE);
    STATE->STATS.INIT_FITNESS = STATE->SWARM.GBEST_FITNESS;
    PSO_HANDLE(NONE, PSO_ERROR_NONE, "INITIAL BEST FITNESS FOR CURRENT SWARM ITERATION: %.6f\n", STATE->SWARM.GBEST_FITNESS);

    // NOW WE CAN ITERATE THROUGH EACH INSTANCE 
    // OF THE ALGORITHM IN ACCORDANCE WITH THE MAX AMOUNT
    // OF ITERATIONS

    // ALL IN ACCORDANCE WITH THE NOVEL VARIANTS ESTABLISHED SUCH AS 
    // GRADUAL INERTIA CONTROL AND EXPLOITATION AND EXPLORATION

    for(int INDEX = 0; INDEX < PSO_MAX_ITER; INDEX++)
    {
        STATE->SWARM.ITERATION = INDEX;
        PSO_UPDATE_ADAPT(STATE);

        // UPDATE ALL OF THE PARTICLES TO LEVERAGE THE NOVEL VARIANTS
        // ITERATES THROUGH EACH INDEX OF THE PARTICLES AND UPDATES 
        for(int ITERATOR = 0; ITERATOR < PSO_MAX_PARTICLES; ITERATOR++)
        {
            PARTICLE P = &STATE->SWARM.PARTICLES[ITERATOR];

            PSO_UPDATE_INERTIA(P, STATE->SWARM.GBEST, STATE->DIMENSIONS,
                            STATE->CURRENT_INERTIA, STATE->CURRENT_COG, STATE->CURRENT_SOC);

            PSO_UPDATE_POS(P, STATE->BOUNDS, STATE->DIMENSIONS);
            P->CURRENT_FITNESS = PSO_DEMAND_FITNESS(P->POSITION, &STATE->DATASET);
            PSO_UPDATE_PBEST(P, STATE->DIMENSIONS);
        }

        PSO_UPDATE_GBEST(STATE);
    }

    STATE->STATS.FINAL_FITNESS = STATE->SWARM.GBEST_FITNESS;
    STATE->STATS.IMPROVEMENT_RATE = (STATE->STATS.INIT_FITNESS - STATE->STATS.FINAL_FITNESS) /
                                    STATE->STATS.INIT_FITNESS * 100.0;

    // NOW WE DETERMINE THE DIFFERENCE BETWEEN THE INITIAL BEST FITNESS
    // AND THE FINAL
    PSO_HANDLE(NONE, PSO_ERROR_NONE, "\n"
        "\nFINAL FITNESS: %.6f\n" 
        "IMPROVEMENT RATE: %.2f%% (FROM %.6f TO %.6f)\n", 
        STATE->STATS.FINAL_FITNESS, STATE->STATS.IMPROVEMENT_RATE,
        STATE->STATS.INIT_FITNESS, STATE->STATS.FINAL_FITNESS);
}

// LOAD THE CSV FILE FOR WORKING WITH TRAIN AND TESTING DATA
int PSO_LOAD_CSV(PSO_DATASET* DATASET, const char* FILENAME)
{
    FILE* FILE_PTR = fopen(FILENAME, "r");
    if(!FILE_PTR) 
    { 
        PSO_ERROR_HANDLE(IO, PSO_ERROR_FILE, "FAILED TO OPEN FILE: %s", FILENAME); 
        return 1;
    }

    // DEFINE SOME REPRESENTATIONS FOR THE DATASET WITHIN EACH CSV
    DATASET->CAPACITY = PSO_MAX_CSV;
    DATASET->SIZE = 0;
    DATASET->DATA = (PSO_DEMAND*)malloc(DATASET->CAPACITY * sizeof(PSO_DEMAND));

    if(!DATASET->DATA)
    {
        fclose(FILE_PTR);
        PSO_ERROR_HANDLE(MEM, PSO_ERROR_MEM, "NO VALID DATA SETS WITHIN THE LOAD CSV FILE: %s", FILE_PTR);
        free(DATASET->DATA);
        return 1;
    }

    // ASSUME THAT THE MAX AMOUNT OF DATA AND OR ITERATIONS
    // WITHIN THE CSV IS LESS THAN THE MAX SWARM COUNT
    if(PSO_MAX_CSV < PSO_MAX_PARTICLES || PSO_MAX_CSV == PSO_MAX_PARTICLES)
    {
        PSO_ERROR_HANDLE(PART, PSO_ERROR_PARTICLE, 
            "COULD NOT PROCESS CSV DATA -> CSV CAPACITY: %d MAX PARTICLES: %d\n", 
            PSO_MAX_CSV, PSO_MAX_PARTICLES);
        return 1;
    }
    
    // MANUALLY READ EACH LINE OF THE CSV
    // PRESUPPOSES THE SIZE OF EACH LINE BEING READ
    // IN RELATION TO EACH NEW LINE WITHIN THE FILE
    char LINE[PSO_CSV_BUFFER];
    fgets(LINE, sizeof(LINE), FILE_PTR);
    
    while(fgets(LINE, sizeof(LINE), FILE_PTR))
    {
        PSO_DEMAND* ENTRY = &DATASET->DATA[DATASET->SIZE];
        
        // MANUALLY MITIGATE THE HEADER LINES
        // THE FOLLOWING WILL LOOK TO DETERMINE WHICH
        // AREAS OF THE HEADER ENCOMPASS ANY AND ALL WHITESPACE
        char* TOKEN = strtok(LINE, ",");
        if(!TOKEN) continue;

        ENTRY->DEMANDS = atof(TOKEN);
        
        // LOOK FOR THE MAX AMOUNT OF INDICATORS
        // AND PARSE THEM - ADD THEM TO THE RESPECTIBVE
        // BUFFER ENTRY
        int VALID_ENTRY = 1;
        for(int i = 0; i < PSO_MAX_IND; i++)
        {
            TOKEN = strtok(NULL, ",");
            if(!VALID_ENTRY) { VALID_ENTRY = 0; break; }

            ENTRY->INDICATIONS[i] = atof(TOKEN);
        }
        
        if(VALID_ENTRY)
        {
            DATASET->SIZE++;
        }
    }
    
    fclose(FILE_PTR);
    PSO_HANDLE(NONE, PSO_ERROR_NONE, 
        "\nLOADED %d RECORDS FROM %s\n", DATASET->SIZE, FILENAME);
    
    return 0;
}
